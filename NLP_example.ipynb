{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM9iuPueJsSjyyQksHiRkBw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rypoko/NLP/blob/main/NLP_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YCAYh25Pmbh"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "from contractions import CONTRACTION_MAP\n",
        "import unicodedata\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtwMy_39Pw2X"
      },
      "source": [
        "seed_urls = ['https://inshorts.com/en/read/business',\n",
        "             'https://inshorts.com/en/read/technology',\n",
        "             'https://inshorts.com/en/read/world']"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeCfIo7AQRrG"
      },
      "source": [
        "def build_dataset(seed_urls):\n",
        "  news_data = []\n",
        "  for url in seed_urls:\n",
        "    news_category = url.split('/')[-1]\n",
        "    data = requests.get(url)\n",
        "    soup = BeautifulSoup(data.content, 'html.parser')\n",
        "\n",
        "    news_articles = [{'news_headline': headline.find('span',\n",
        "                                                     attrs={'itemprop': 'headline'}).string,\n",
        "                      'news_article': article.find('div',\n",
        "                                                   attrs={'itemprop': 'articleBody'}).string,\n",
        "                      'news_category': news_category}\n",
        "                     \n",
        "                     for headline, article in zip(soup.find_all('div',\n",
        "                                                                class_=['news-card-title news-right-box']),\n",
        "                                                  soup.find_all('div',\n",
        "                                                                class_=['news-card-content news-right-box']))\n",
        "                     ]\n",
        "    news_data.extend(news_articles)\n",
        "\n",
        "  df = pd.DataFrame(news_data)\n",
        "  df = df[['news_headline', 'news_article', 'news_category']]\n",
        "  return df"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "iUCQeq2MTpxU",
        "outputId": "7e534254-44e4-4373-ab6e-914b32bb58fb"
      },
      "source": [
        "news_df = build_dataset(seed_urls)\n",
        "news_df.head(10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>news_headline</th>\n",
              "      <th>news_article</th>\n",
              "      <th>news_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>India-born buyer of ₹504 cr art couldn't affor...</td>\n",
              "      <td>Indian-origin Vignesh Sundaresan and Anand Ven...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Boats honk in celebration after cargo ship stu...</td>\n",
              "      <td>A video has surfaced on social media showing t...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>World Bank clears ₹9.6k cr loan to Pak days af...</td>\n",
              "      <td>The World Bank has signed agreements with Paki...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What could soon be in short supply due to the ...</td>\n",
              "      <td>The Suez Canal jam could soon cause shortages ...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Egypt Prez orders to prepare for 'third scenar...</td>\n",
              "      <td>Egyptian President Abdel Fattah Al-Sisi has or...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Latest satellite pics show excavation of sand ...</td>\n",
              "      <td>The latest satellite pictures by Maxar Technol...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Adani to buy Warora-Kurnool Transmission Limit...</td>\n",
              "      <td>Adani Transmission said it has entered into a ...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Disinvestment target of FY22 achievable, LIC I...</td>\n",
              "      <td>Chief Economic Adviser (CEA) KV Subramanian on...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NPCI issues guidelines for capping market shar...</td>\n",
              "      <td>The NPCI has issued guidelines for capping UPI...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>US cryptocurrency exchange Coinbase to set up ...</td>\n",
              "      <td>US cryptocurrency exchange Coinbase said the c...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       news_headline  ... news_category\n",
              "0  India-born buyer of ₹504 cr art couldn't affor...  ...      business\n",
              "1  Boats honk in celebration after cargo ship stu...  ...      business\n",
              "2  World Bank clears ₹9.6k cr loan to Pak days af...  ...      business\n",
              "3  What could soon be in short supply due to the ...  ...      business\n",
              "4  Egypt Prez orders to prepare for 'third scenar...  ...      business\n",
              "5  Latest satellite pics show excavation of sand ...  ...      business\n",
              "6  Adani to buy Warora-Kurnool Transmission Limit...  ...      business\n",
              "7  Disinvestment target of FY22 achievable, LIC I...  ...      business\n",
              "8  NPCI issues guidelines for capping market shar...  ...      business\n",
              "9  US cryptocurrency exchange Coinbase to set up ...  ...      business\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbuIkOMGUXcd",
        "outputId": "1eeea11e-5fe3-451d-e451-a8b56816569e"
      },
      "source": [
        "news_df.news_category.value_counts()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "technology    25\n",
              "world         24\n",
              "business      24\n",
              "Name: news_category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpIoybFhyLMT",
        "outputId": "809d15f8-6832-470f-81af-c175371e1aa0"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm', parse=True, tag=True, entity=True)\n",
        "#nlp_vec = spacy.load('en_vecs', parse = True, tag=True, entity=True)\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "T5O_yNLUBts6",
        "outputId": "98422e2e-98b0-406a-e54f-2265e9e682de"
      },
      "source": [
        "#strip HTML\n",
        "def strip_html_tags(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  stripped_text = soup.get_text()\n",
        "  return stripped_text\n",
        "\n",
        "strip_html_tags('<html><h2>Some important text</h2></html>')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Some important text'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4a8ENiMKCSJw",
        "outputId": "8d621c46-f158-48cb-d8cb-a92949cc42c6"
      },
      "source": [
        "#strip accents\n",
        "def remove_accented_chars(text):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text\n",
        "\n",
        "remove_accented_chars('Sómě Áccěntěd těxt')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Some Accented text'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gq8jHrpuDGKQ",
        "outputId": "88570703-a724-4505-e90a-3d55c4b5eb6c"
      },
      "source": [
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text\n",
        "\n",
        "expand_contractions(\"Y'all can't expand contractions I'd think\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'You all cannot expand contractions I would think'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2s2N_CCbRDl5",
        "outputId": "50141136-5e1f-458d-f827-5d8bd2a5e23d"
      },
      "source": [
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "remove_special_characters(\"Well this was fun! What do you think? 123#@!\", \n",
        "                          remove_digits=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Well this was fun What do you think '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XCYod2fHRnTH",
        "outputId": "28b1d437-4fcb-45e3-e6f3-1f8e244d98da"
      },
      "source": [
        "def simple_stemmer(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'My system keep crash hi crash yesterday, our crash daili'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LBodjlvASMDG",
        "outputId": "7282baf9-c1d8-40ce-a3ee-daf1b95b453b"
      },
      "source": [
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "\n",
        "lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'My system keep crash ! his crash yesterday , ours crash daily'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ovqEbi3MSh_K",
        "outputId": "e9e89bea-3ebb-41e8-ef54-960cb912ab56"
      },
      "source": [
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "', , stopwords , computer not'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS-5ldQyTJF2"
      },
      "source": [
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
        "                     accented_char_removal=True, text_lower_case=True, \n",
        "                     text_lemmatization=True, special_char_removal=True, \n",
        "                     stopword_removal=True, remove_digits=True):\n",
        "    \n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # strip HTML\n",
        "        if html_stripping:\n",
        "            doc = strip_html_tags(doc)\n",
        "        # remove accented characters\n",
        "        if accented_char_removal:\n",
        "            doc = remove_accented_chars(doc)\n",
        "        # expand contractions    \n",
        "        if contraction_expansion:\n",
        "            doc = expand_contractions(doc)\n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        # lemmatize text\n",
        "        if text_lemmatization:\n",
        "            doc = lemmatize_text(doc)\n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        # remove stopwords\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
        "            \n",
        "        normalized_corpus.append(doc)\n",
        "        \n",
        "    return normalized_corpus"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qx19P9aLTvFn",
        "outputId": "38a157fc-75df-43c2-c60c-37bbc581a512"
      },
      "source": [
        "# combining headline and article text\n",
        "news_df['full_text'] = news_df[\"news_headline\"].map(str)+ '. ' + news_df[\"news_article\"]\n",
        "\n",
        "# pre-process text and store the same\n",
        "news_df['clean_text'] = normalize_corpus(news_df['full_text'])\n",
        "norm_corpus = list(news_df['clean_text'])\n",
        "\n",
        "# show a sample news article\n",
        "news_df.iloc[1][['full_text', 'clean_text']].to_dict()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clean_text': 'boat honk celebration cargo ship stick suez canal move slightly video surface social medium show tugboat suez canal honking celebration able slightly move massive container ship stick suez canal tonne sand dredge tugboat pull push ship try dislodge ever give ship weigh ton',\n",
              " 'full_text': 'Boats honk in celebration after cargo ship stuck in Suez Canal moves slightly. A video has surfaced on social media showing tugboats in the Suez Canal honking in celebration after they were able to slightly move the massive container ship stuck in Suez Canal. About 20,000 tonnes of sand was dredged and 14 tugboats pulled and pushed the ship to try to dislodge it. The Ever Given ship weighs 2,20,000 tons.'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dUvmi5LVA3D"
      },
      "source": [
        "news_df.to_csv('news.csv', index=False, encoding='utf-8')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "tGyv-T3DWIV2",
        "outputId": "d5bf1da2-08f1-4fa2-f09f-ef77dc7b5df1"
      },
      "source": [
        "# create a basic pre-processed corpus, don't lowercase to get POS context\n",
        "corpus = normalize_corpus(news_df['full_text'], text_lower_case=False, \n",
        "                          text_lemmatization=False, special_char_removal=False)\n",
        "\n",
        "# demo for POS tagging for sample news headline\n",
        "sentence = str(news_df.iloc[1].news_headline)\n",
        "sentence_nlp = nlp(sentence)\n",
        "\n",
        "# POS tagging with Spacy \n",
        "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
        "pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])\n",
        "\n",
        "# POS tagging with nltk\n",
        "#nltk_pos_tagged = nltk.pos_tag(sentence.split())\n",
        "#pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag'])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>POS tag</th>\n",
              "      <th>Tag type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Boats</td>\n",
              "      <td>NNS</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>honk</td>\n",
              "      <td>VBP</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>in</td>\n",
              "      <td>IN</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>celebration</td>\n",
              "      <td>NN</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>after</td>\n",
              "      <td>IN</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>cargo</td>\n",
              "      <td>NN</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ship</td>\n",
              "      <td>NN</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>stuck</td>\n",
              "      <td>VBD</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>in</td>\n",
              "      <td>IN</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Suez</td>\n",
              "      <td>NNP</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Canal</td>\n",
              "      <td>NNP</td>\n",
              "      <td>PROPN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>moves</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>VERB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>slightly</td>\n",
              "      <td>RB</td>\n",
              "      <td>ADV</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Word POS tag Tag type\n",
              "0         Boats     NNS     NOUN\n",
              "1          honk     VBP     VERB\n",
              "2            in      IN      ADP\n",
              "3   celebration      NN     NOUN\n",
              "4         after      IN      ADP\n",
              "5         cargo      NN     NOUN\n",
              "6          ship      NN     NOUN\n",
              "7         stuck     VBD     VERB\n",
              "8            in      IN      ADP\n",
              "9          Suez     NNP    PROPN\n",
              "10        Canal     NNP    PROPN\n",
              "11        moves     VBZ     VERB\n",
              "12     slightly      RB      ADV"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BpBNHdCWvJf",
        "outputId": "395e4c60-6bbf-4bf4-f0cb-54ffc3df154f"
      },
      "source": [
        "from nltk.corpus import conll2000\n",
        "\n",
        "data = conll2000.chunked_sents()\n",
        "train_data = data[:10900]\n",
        "test_data = data[10900:] \n",
        "\n",
        "print(len(train_data), len(test_data))\n",
        "print(train_data[1]) "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10900 48\n",
            "(S\n",
            "  Chancellor/NNP\n",
            "  (PP of/IN)\n",
            "  (NP the/DT Exchequer/NNP)\n",
            "  (NP Nigel/NNP Lawson/NNP)\n",
            "  (NP 's/POS restated/VBN commitment/NN)\n",
            "  (PP to/TO)\n",
            "  (NP a/DT firm/NN monetary/JJ policy/NN)\n",
            "  (VP has/VBZ helped/VBN to/TO prevent/VB)\n",
            "  (NP a/DT freefall/NN)\n",
            "  (PP in/IN)\n",
            "  (NP sterling/NN)\n",
            "  (PP over/IN)\n",
            "  (NP the/DT past/JJ week/NN)\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obtdcjZRXgV-"
      },
      "source": [
        "from nltk import conlltags2tree, tree2conlltags\n",
        "def conll_tag_chunks(chunk_sents):\n",
        "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
        "    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
        "\n",
        "\n",
        "def combined_tagger(train_data, taggers, backoff=None):\n",
        "    for tagger in taggers:\n",
        "        backoff = tagger(train_data, backoff=backoff)\n",
        "    return backoff "
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yHtU8L6Xra2",
        "outputId": "14840c33-47ed-49e1-9869-568b945675a9"
      },
      "source": [
        "from nltk.tag import UnigramTagger, BigramTagger\n",
        "from nltk.chunk import ChunkParserI\n",
        "\n",
        "# define the chunker class\n",
        "class NGramTagChunker(ChunkParserI):\n",
        "    \n",
        "  def __init__(self, train_sentences, tagger_classes=[UnigramTagger, BigramTagger]):\n",
        "    train_sent_tags = conll_tag_chunks(train_sentences)\n",
        "    self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
        "\n",
        "  def parse(self, tagged_sentence):\n",
        "    if not tagged_sentence: \n",
        "        return None\n",
        "    pos_tags = [tag for word, tag in tagged_sentence]\n",
        "    chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
        "    chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
        "    wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
        "                     in zip(tagged_sentence, chunk_tags)]\n",
        "    return conlltags2tree(wpc_tags)\n",
        "  \n",
        "# train chunker model  \n",
        "ntc = NGramTagChunker(train_data)\n",
        "\n",
        "# evaluate chunker model performance\n",
        "print(ntc.evaluate(test_data))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  90.0%%\n",
            "    Precision:     82.1%%\n",
            "    Recall:        86.3%%\n",
            "    F-Measure:     84.1%%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcmXCpehYson",
        "outputId": "db216897-c158-480a-a950-b8e7b076d897"
      },
      "source": [
        "chunk_tree = ntc.parse(nltk_pos_tagged)\n",
        "print(chunk_tree)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP Boats/NNS)\n",
            "  (VP honk/VBP)\n",
            "  (PP in/IN)\n",
            "  (NP celebration/NN)\n",
            "  (PP after/IN)\n",
            "  (NP cargo/NN ship/NN)\n",
            "  (VP stuck/VBN)\n",
            "  (PP in/IN)\n",
            "  (NP Suez/NNP Canal/NNP)\n",
            "  (VP moves/VBZ slightly/RB))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "dwbJ8eU8YyRq",
        "outputId": "53400d78-43b8-48e5-9038-6c1d70852319"
      },
      "source": [
        "from IPython.display import display\n",
        "\n",
        "## download and install ghostscript from https://www.ghostscript.com/download/gsdnld.html\n",
        "\n",
        "# often need to add to the path manually (for windows)\n",
        "os.environ['PATH'] = os.environ['PATH']+\";C:\\\\Program Files\\\\gs\\\\gs9.09\\\\bin\\\\\"\n",
        "\n",
        "display(chunk_tree)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0m_canvas_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/draw/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, **kw)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If no parent was given, set up a top-level window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLTK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<Control-p>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', [('Boats', 'NNS')]), Tree('VP', [('honk', 'VBP')]), Tree('PP', [('in', 'IN')]), Tree('NP', [('celebration', 'NN')]), Tree('PP', [('after', 'IN')]), Tree('NP', [('cargo', 'NN'), ('ship', 'NN')]), Tree('VP', [('stuck', 'VBN')]), Tree('PP', [('in', 'IN')]), Tree('NP', [('Suez', 'NNP'), ('Canal', 'NNP')]), Tree('VP', [('moves', 'VBZ'), ('slightly', 'RB')])])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}